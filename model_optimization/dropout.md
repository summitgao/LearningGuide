# Dropout

## 1、Dropout

没有添加Dropout的网络是需要对网络的全部节点进行学习的，而添加了Dropout的网络只需要对其中没有被替换为掩码的节点进行训练，如下图所示。Dropout能够有效解决模型的过拟合问题，从而使得训练更深、更宽的网络成为可能。

<figure><img src="https://img-blog.csdnimg.cn/eee92cbe2b3646ff82ec96bc914038f3.jpeg" alt=""><figcaption></figcaption></figure>

在Dropout出现之前，正则化是主要的用来解决模型过拟合的策略，如L1正则和L2正则。但是它们并没有完全解决模型的过拟合问题，原因就是网络中存在共适应（co-adaption）问题。**所谓共适应，是指网络中的一些节点会比另外一些节点有更强的表征能力。**这时，随着网络的不断训练，具有更强表征能力的节点被不断强化，而表征能力更弱的节点则被不断弱化，直到对网络的贡献可以忽略不计。这时候网络中只有部分节点被训练，浪费了网络的宽度和深度，进而导致模型的效果提升受到限制。

而Dropout解决了共适应问题，使得训练更宽的网络成为可能。

使用Dropout的几个技巧如下：

（1）当丢失率为0.5时，Dropout会有最强的正则效果。因为p(1-p)在p=0.5时取得最大值。

（2）丢失率的选择策略：在比较深的网络中，使用0.5的丢失率是比较好的选择，因为这时Dropout能取到最好的正则效果。在比较浅的网络中，丢失率应该低于0.2，因为过大的丢失率会导致丢失过多的输入数据，对模型的影响比较大；不建议使用大于0.5的丢失率，因为它在丢失过多节点的情况下并不会取得更好的正则效果。

（3）Dropout 应用在全连接层中，实际使用时要注意网络的 train() 和 eval() 模式。

Hinton等人认为由于Dropout在训练过程中会随机丢弃节点，因此会在训练过程中产生大量不同的网络，而最终的网络相当于这些随机网络的模型集成。

## 2、DropBlock

不同于MLP的特征图是一个特征向量，CNN的特征图是一个由宽、高、通道数组成的三维矩阵，因此需要针对CNN的特点单独设计Dropout。

在CNN中，我们可以以通道为单位来随机丢弃，这样可以增加其他通道的建模能力并缓解通道之间的共适应问题，这个策略叫作空间（spatial）Dropout。我们也可以随机丢弃特征图中的一大块区域，来避免临近像素的互相弥补，这个方法叫作DropBlock。还有一个常见的策略叫作最大池化（max pooling）Dropout，它的计算方式是在执行最大池化之前，将窗口内的像素随机替换为掩码，这样使得窗口内较小的值也有机会影响网络的性能。空间Dropout、DropBlock和最大池化Dropout的可视化如下图所示。

<figure><img src="https://img-blog.csdnimg.cn/936fd46564944430881888245302b514.jpeg" alt=""><figcaption></figcaption></figure>

\
